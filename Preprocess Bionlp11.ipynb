{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import gzip\n",
    "from genia_tokenizer import tokenize as genia_tokenize\n",
    "from os.path import join\n",
    "import glob\n",
    "import re\n",
    "import os\n",
    "import spacy\n",
    "from spacy.gold import biluo_tags_from_offsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nlp = spacy.load('en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "DIGITS_RE = re.compile(r'(.*)\\/(\\d+)\\.(.*)')\n",
    "bionlp_dir = '/home/rohit/Documents/Spring_2018/Team_Project/data/bionlp09/'\n",
    "ANN_RE = re.compile(r'.*Protein (\\d+) (\\d+) (.*)$')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3203 800 800 800\n"
     ]
    }
   ],
   "source": [
    "total_files = glob.glob(join(bionlp_dir, '*'))\n",
    "stoken_files = glob.glob(join(bionlp_dir, '*stoken'))\n",
    "a1_files = glob.glob(join(bionlp_dir, '*a1'))\n",
    "txt_files = glob.glob(join(bionlp_dir, '*txt'))\n",
    "print(len(total_files), len(a1_files), len(txt_files), len(stoken_files))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def biluo_tags_from_offsets(text, entities, tokens, missing='O'):\n",
    "    \"\"\"Encode labelled spans into per-token tags, using the\n",
    "    Begin/In/Last/Unit/Out scheme (BILUO).\n",
    "    doc (Doc): The document that the entity offsets refer to. The output tags\n",
    "        will refer to the token boundaries within the document.\n",
    "    entities (iterable): A sequence of `(start, end, label)` triples. `start`\n",
    "        and `end` should be character-offset integers denoting the slice into\n",
    "        the original string.\n",
    "    RETURNS (list): A list of unicode strings, describing the tags. Each tag\n",
    "        string will be of the form either \"\", \"O\" or \"{action}-{label}\", where\n",
    "        action is one of \"B\", \"I\", \"L\", \"U\". The string \"-\" is used where the\n",
    "        entity offsets don't align with the tokenization in the `Doc` object.\n",
    "        The training algorithm will view these as missing values. \"O\" denotes a\n",
    "        non-entity token. \"B\" denotes the beginning of a multi-token entity,\n",
    "        \"I\" the inside of an entity of three or more tokens, and \"L\" the end\n",
    "        of an entity of two or more tokens. \"U\" denotes a single-token entity.\n",
    "    EXAMPLE:\n",
    "        >>> text = 'I like London.'\n",
    "        >>> entities = [(len('I like '), len('I like London'), 'LOC')]\n",
    "        >>> doc = nlp.tokenizer(text)\n",
    "        >>> tags = biluo_tags_from_offsets(doc, entities)\n",
    "        >>> assert tags == ['O', 'O', 'U-LOC', 'O']\n",
    "    \"\"\"\n",
    "    starts = {token.idx: token.i for i, token in enumerate(entities)}\n",
    "    ends = {token.idx+len(token): token.i for token in doc}\n",
    "    biluo = ['-' for _ in doc]\n",
    "    # Handle entity cases\n",
    "    for start_char, end_char, label in entities:\n",
    "        start_token = starts.get(start_char)\n",
    "        end_token = ends.get(end_char)\n",
    "        # Only interested if the tokenization is correct\n",
    "        if start_token is not None and end_token is not None:\n",
    "            if start_token == end_token:\n",
    "                biluo[start_token] = 'U-%s' % label\n",
    "            else:\n",
    "                biluo[start_token] = 'B-%s' % label\n",
    "                for i in range(start_token+1, end_token):\n",
    "                    biluo[i] = 'I-%s' % label\n",
    "                biluo[end_token] = 'L-%s' % label\n",
    "    # Now distinguish the O cases from ones where we miss the tokenization\n",
    "    entity_chars = set()\n",
    "    for start_char, end_char, label in entities:\n",
    "        for i in range(start_char, end_char):\n",
    "            entity_chars.add(i)\n",
    "    for token in doc:\n",
    "        for i in range(token.idx, token.idx+len(token)):\n",
    "            if i in entity_chars:\n",
    "                break\n",
    "        else:\n",
    "            biluo[token.i] = missing\n",
    "    return biluo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert 7 digit IDS to digit IDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 3200 0\n"
     ]
    }
   ],
   "source": [
    "sevens = eights = outsiders = 0\n",
    "for filename in total_files:\n",
    "    m = DIGITS_RE.match(filename)\n",
    "    if not m:\n",
    "        continue\n",
    "        \n",
    "    if len(m.group(2)) == 7:\n",
    "        sevens += 1\n",
    "        new_id = '0' + m.group(2)\n",
    "        new_filename = m.group(1) + '/' + new_id + '.' + m.group(3)\n",
    "        os.rename(filename, new_filename)\n",
    "    elif len(m.group(2)) == 8:\n",
    "        eights += 1\n",
    "    else:\n",
    "        outsiders += 1\n",
    "    \n",
    "print(sevens, eights, outsiders)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text to annotation file maps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "stoken2ann_fnames = {}\n",
    "for stoken_file in stoken_files:\n",
    "    m = DIGITS_RE.match(stoken_file)\n",
    "    stoken_pid = m.group(2)\n",
    "    for a1_file in a1_files:\n",
    "        m = DIGITS_RE.match(a1_file)\n",
    "        ann_pid = m.group(2)\n",
    "        if ann_pid == stoken_pid:\n",
    "            break\n",
    "    stoken2ann_fnames[stoken_file] = a1_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for f_idx, (stoken_file, a1_file) in enumerate(stoken2ann_fnames.items()):\n",
    "    if f_idx == 0:\n",
    "        continue\n",
    "    print(stoken_file, a1_file)\n",
    "    abst = ''\n",
    "    with open(stoken_file, 'r') as f_stoken:\n",
    "        for line_idx, line in enumerate(f_stoken):\n",
    "            line = line.strip()\n",
    "            abst += line + ' '\n",
    "            if line_idx == 0:\n",
    "                abst += '\\n'\n",
    "            tokens = genia_tokenize(line)\n",
    "            print(tokens)\n",
    "    doc = nlp(abst)\n",
    "    anns = []\n",
    "    with open(a1_file, 'r') as f_a1:\n",
    "        for line in f_a1:\n",
    "            line = line.strip()\n",
    "            _, ann, mention = line.split('\\t')\n",
    "            _, start, end = ann.split()\n",
    "            start, end = int(start), int(end)\n",
    "            anns.append((start, end, 'Protein'))\n",
    "    print(anns, biluo_tags_from_offsets(doc, anns))\n",
    "    break\n",
    "    if f_idx == 5:\n",
    "        break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
